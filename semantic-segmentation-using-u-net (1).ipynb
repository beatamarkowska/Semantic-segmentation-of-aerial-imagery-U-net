{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1196732,"sourceType":"datasetVersion","datasetId":681625}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Semantic segmentation of aerial imagery using U-Net","metadata":{}},{"cell_type":"markdown","source":"By employing a U-Net architecture and patchifying aerial images into **256x256x3** tensors, the model achieved notable performance, demonstrating a **test accuracy of 87%**, a Jaccard coefficient of 0.7308, and a loss of 0.8974. These results suggest strong generalization ability, as the model also exhibited a **validation accuracy of 84%**, a validation Jaccard coefficient of 0.6869, and a validation loss of 0.9149. These metrics indicate the model's proficiency in segmenting aerial imagery, effectively classifying pixels into distinct object categories. \n\n*     Building: #3C1098\n*     Land (unpaved area): #8429F6\n*     Road: #6EC1E4\n*     Vegetation: #FEDD3A\n*     Water: #E2A929\n*     Unlabeled: #9B9B9B","metadata":{}},{"cell_type":"markdown","source":"# 01. Enable GPU as the physical device","metadata":{}},{"cell_type":"markdown","source":"It's essential to enable GPU acceleration within the Kaggle notebook environment. This is achieved by navigating to the right-side panel of the notebook and selecting \"Session options\"  Within the Session options, choose \"GPU T4 x2\" as the ACCELERATOR. \nFor more informations refer [enable GPU T4 x2 as the ACCELERATOR](https://www.kaggle.com/docs/notebooks#settings) ","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\nprint(\"TensorFlow version:\", tf.__version__)\n\n# List GPUs available\ngpus = tf.config.list_physical_devices('GPU')\nprint(\"GPUs:\", gpus)\n\n# Check if GPUs are available\nif gpus:\n    print(\"GPU is available.\")\nelse:\n    print(\"GPU is not available.\")\n\n# Set memory growth for each GPU\nfor gpu in gpus:\n    tf.config.experimental.set_memory_growth(gpu, True)\n","metadata":{"id":"eawVpAONy7XQ","outputId":"c94ed6f6-a51b-4f06-f86d-a190edeaaf46","execution":{"iopub.status.busy":"2025-05-12T11:17:37.973709Z","iopub.execute_input":"2025-05-12T11:17:37.974060Z","iopub.status.idle":"2025-05-12T11:17:49.995117Z","shell.execute_reply.started":"2025-05-12T11:17:37.974020Z","shell.execute_reply":"2025-05-12T11:17:49.994220Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 02. Install Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install matplotlib\n!pip install scikit-learn\n!pip install -U segmentation-models\n!pip install patchify\n!pip install Pillow","metadata":{"id":"IAO8JPV7rBXz","outputId":"067ead7c-9066-45b5-ed0b-f3e6cfc939a0","execution":{"iopub.status.busy":"2025-05-12T11:17:53.092607Z","iopub.execute_input":"2025-05-12T11:17:53.093158Z","iopub.status.idle":"2025-05-12T11:18:38.735794Z","shell.execute_reply.started":"2025-05-12T11:17:53.093131Z","shell.execute_reply":"2025-05-12T11:18:38.734752Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 03. Patchify images and masks","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\n\nimport os\nos.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n\nfrom matplotlib import pyplot as plt\nfrom patchify import patchify\nfrom PIL import Image\nimport segmentation_models as sm\nfrom tensorflow.keras.metrics import MeanIoU\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport random","metadata":{"id":"WQYbq0AMq6UM","outputId":"526e20d0-04c9-4093-b729-c0d8eb5cad1a","execution":{"iopub.status.busy":"2025-05-12T11:18:38.737655Z","iopub.execute_input":"2025-05-12T11:18:38.737945Z","iopub.status.idle":"2025-05-12T11:18:39.304908Z","shell.execute_reply.started":"2025-05-12T11:18:38.737920Z","shell.execute_reply":"2025-05-12T11:18:39.304252Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_root_folder = \"/kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:18:39.305848Z","iopub.execute_input":"2025-05-12T11:18:39.306519Z","iopub.status.idle":"2025-05-12T11:18:39.310243Z","shell.execute_reply.started":"2025-05-12T11:18:39.306496Z","shell.execute_reply":"2025-05-12T11:18:39.309451Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_name = 'Semantic segmentation dataset'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:18:55.011179Z","iopub.execute_input":"2025-05-12T11:18:55.011815Z","iopub.status.idle":"2025-05-12T11:18:55.015511Z","shell.execute_reply.started":"2025-05-12T11:18:55.011781Z","shell.execute_reply":"2025-05-12T11:18:55.014664Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.walk(dataset_root_folder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:18:58.212163Z","iopub.execute_input":"2025-05-12T11:18:58.212613Z","iopub.status.idle":"2025-05-12T11:18:58.218864Z","shell.execute_reply.started":"2025-05-12T11:18:58.212583Z","shell.execute_reply":"2025-05-12T11:18:58.218045Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for path, subdirs, files in os.walk(dataset_root_folder):\n  dir_name = path.split(os.path.sep)[-1]\n  print(dir_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:19:00.763623Z","iopub.execute_input":"2025-05-12T11:19:00.763937Z","iopub.status.idle":"2025-05-12T11:19:00.897238Z","shell.execute_reply.started":"2025-05-12T11:19:00.763914Z","shell.execute_reply":"2025-05-12T11:19:00.896413Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_folder = f\"{dataset_root_folder}/Tile 1/images\"\nimage_files = [f for f in os.listdir(image_folder) if f.endswith('.jpg')]\nnum_images_to_show = min(20, len(image_files))\nfor i in range(num_images_to_show):\n    image_path = os.path.join(image_folder, image_files[i])\n    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n    print(f\"Obraz {image_files[i]} has shape: {image.shape}\")\n   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:19:04.780732Z","iopub.execute_input":"2025-05-12T11:19:04.781128Z","iopub.status.idle":"2025-05-12T11:19:04.911799Z","shell.execute_reply.started":"2025-05-12T11:19:04.781088Z","shell.execute_reply":"2025-05-12T11:19:04.910941Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The images have different sizes that need to be unified.\nTile and mask procesing:\n\n1. Choosing path size to 256 or 512\n2. Make all tiles and masks images sizes is the multiple of patch size\n3. Split all the images into patch sizes and convert them into numpy array\n\nProcessing was presented on one image and will be performed on the entire data set later.","metadata":{}},{"cell_type":"code","source":"image = cv2.imread(f\"{dataset_root_folder}/Tile 1/images/image_part_001.jpg\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:19:08.847973Z","iopub.execute_input":"2025-05-12T11:19:08.848600Z","iopub.status.idle":"2025-05-12T11:19:08.856632Z","shell.execute_reply.started":"2025-05-12T11:19:08.848571Z","shell.execute_reply":"2025-05-12T11:19:08.855668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:19:11.727663Z","iopub.execute_input":"2025-05-12T11:19:11.728351Z","iopub.status.idle":"2025-05-12T11:19:11.735043Z","shell.execute_reply.started":"2025-05-12T11:19:11.728317Z","shell.execute_reply":"2025-05-12T11:19:11.733753Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:19:17.092420Z","iopub.execute_input":"2025-05-12T11:19:17.093047Z","iopub.status.idle":"2025-05-12T11:19:17.098066Z","shell.execute_reply.started":"2025-05-12T11:19:17.093021Z","shell.execute_reply":"2025-05-12T11:19:17.097049Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(image)\nplt.title(\"Tile 1 - image_part_001.jpg BGR\")\nplt.axis(\"on\")\nplt.show()\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:19:20.692504Z","iopub.execute_input":"2025-05-12T11:19:20.692844Z","iopub.status.idle":"2025-05-12T11:19:21.145498Z","shell.execute_reply.started":"2025-05-12T11:19:20.692822Z","shell.execute_reply":"2025-05-12T11:19:21.144478Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nplt.imshow(image_rgb)\nplt.title(\"Tile 1 - image_part_001.jpg RGB\")\nplt.axis(\"on\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:19:28.863256Z","iopub.execute_input":"2025-05-12T11:19:28.864188Z","iopub.status.idle":"2025-05-12T11:19:29.262818Z","shell.execute_reply.started":"2025-05-12T11:19:28.864156Z","shell.execute_reply":"2025-05-12T11:19:29.261970Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Choosing patch size: 256 or 512?\n\nCompatibility with neural network architectures: 256 or 512 are popular sizes because they correspond to the resolutions used by many popular neural network architectures such as ResNet, VGG, U-Net, etc. Many of these models are designed for image sizes such as 224x224, 256x256, or 512x512 because they are easy to scale to at the architecture level.\n\nComputational power: These sizes often provide a good balance between accuracy and computation time. They provide enough information about the image structure without putting too much strain on the GPU memory, which is especially important for tasks such as image segmentation.\n\nDivisibility by layer sizes in models: 256 and 512 are numbers that are easily divisible by 2 (e.g. 512 is 2^9, 256 is 2^8). The divisibility by 2 is important because during processing in a neural network (e.g. in convolutional networks), the image is often reduced to a smaller size by operations such as pooling or downsampling.\nNeural networks such as U-Net, ResNet or VGG are designed in such a way that the size of the input image should fit into their architecture, where the image is gradually reduced (e.g. by convolutional and pooling layers) and then rebuilt (e.g. by transposed layers). Choosing a patch size of 256 or 512 ensures that the image easily fits into this process.\n\nComputational efficiency (GPU memory): A larger patch size (e.g. 512x512) can offer more details in a single image, but a larger patch size means more GPU memory is required. A larger patch means that the network has to process more data in one pass, which can slow down training, especially if you have limited GPU memory.\nA smaller patch size (e.g. 256x256) means less memory, which can speed up computation, but can reduce detail if the patch is too small for the required image analysis.\n\nAveraged results and stability: 256x256 and 512x512 are popular sizes because they help to obtain stable results in image analysis, especially when images are divided into smaller pieces (patches). A patch size that is too large can contain too much information, which can make the model harder to train or unstable.\n\nOn the other hand, a patch that is too small (e.g. 128x128 or 64x64) can contain too little information to correctly recognize patterns and features in the image, which can make accurate segmentation difficult.\n\nSpecific task: In image segmentation or image analysis (e.g. in satellite image processing, medical image processing, etc.), the patch size also depends on the scale and characteristics of the objects we want to detect. Often a 256x256 or 512x512 patch is adequate to capture the structures in the image, but not too large to include information that may be irrelevant or redundant.","metadata":{}},{"cell_type":"markdown","source":"256x256 - A popular patch size in neural networks, especially in medical segmentation (e.g. MRI, CT images) where details need to be captured at different levels.\n\n512x512 - More commonly used for larger images where we want the patch to contain more context (e.g. satellite images).\n\n256 or 512? If you care about training speed and have limited GPU memory, 256x256 is a good compromise.\n\nIf you care about accuracy and have enough processing power (e.g. large GPU memory), 512x512 may be more appropriate.","metadata":{}},{"cell_type":"code","source":"image_patch_size = 256","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:19:41.053149Z","iopub.execute_input":"2025-05-12T11:19:41.053489Z","iopub.status.idle":"2025-05-12T11:19:41.057556Z","shell.execute_reply.started":"2025-05-12T11:19:41.053462Z","shell.execute_reply":"2025-05-12T11:19:41.056523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"size_x = (image.shape[1] // image_patch_size) * image_patch_size\nsize_y = (image.shape[0] // image_patch_size) * image_patch_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:19:46.259271Z","iopub.execute_input":"2025-05-12T11:19:46.260105Z","iopub.status.idle":"2025-05-12T11:19:46.263974Z","shell.execute_reply.started":"2025-05-12T11:19:46.260065Z","shell.execute_reply":"2025-05-12T11:19:46.263081Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The model requires uniform image slices to ensure consistency in the neural network processing). Adjusting the image dimensions to a size that is a multiple of the \"patch\" size. The goal is for the image to be able to be divided into equal pieces (patches) without any leftovers at the edges of the image. The size_x and size_y values ​​are used to crop the image so that its dimensions are a multiple of image_patch_size.","metadata":{}},{"cell_type":"code","source":"image1 = Image.fromarray(image)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:22:37.749440Z","iopub.execute_input":"2025-05-12T11:22:37.749793Z","iopub.status.idle":"2025-05-12T11:22:37.754382Z","shell.execute_reply.started":"2025-05-12T11:22:37.749769Z","shell.execute_reply":"2025-05-12T11:22:37.753629Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(type(image))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:22:40.833578Z","iopub.execute_input":"2025-05-12T11:22:40.833885Z","iopub.status.idle":"2025-05-12T11:22:40.838659Z","shell.execute_reply.started":"2025-05-12T11:22:40.833863Z","shell.execute_reply":"2025-05-12T11:22:40.837708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(type(image1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:22:44.699132Z","iopub.execute_input":"2025-05-12T11:22:44.699810Z","iopub.status.idle":"2025-05-12T11:22:44.704197Z","shell.execute_reply.started":"2025-05-12T11:22:44.699781Z","shell.execute_reply":"2025-05-12T11:22:44.703291Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image2 = image1.crop((0,0,size_x,size_y))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:22:48.940624Z","iopub.execute_input":"2025-05-12T11:22:48.940940Z","iopub.status.idle":"2025-05-12T11:22:48.945934Z","shell.execute_reply.started":"2025-05-12T11:22:48.940917Z","shell.execute_reply":"2025-05-12T11:22:48.944995Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image2.size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:22:52.251301Z","iopub.execute_input":"2025-05-12T11:22:52.251897Z","iopub.status.idle":"2025-05-12T11:22:52.257276Z","shell.execute_reply.started":"2025-05-12T11:22:52.251868Z","shell.execute_reply":"2025-05-12T11:22:52.256252Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(image2)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:22:54.675463Z","iopub.execute_input":"2025-05-12T11:22:54.676271Z","iopub.status.idle":"2025-05-12T11:22:55.031317Z","shell.execute_reply.started":"2025-05-12T11:22:54.676229Z","shell.execute_reply":"2025-05-12T11:22:55.030453Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image3 = np.array(image2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:22:59.023933Z","iopub.execute_input":"2025-05-12T11:22:59.024514Z","iopub.status.idle":"2025-05-12T11:22:59.029470Z","shell.execute_reply.started":"2025-05-12T11:22:59.024485Z","shell.execute_reply":"2025-05-12T11:22:59.028448Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image3.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:23:02.291433Z","iopub.execute_input":"2025-05-12T11:23:02.292196Z","iopub.status.idle":"2025-05-12T11:23:02.297069Z","shell.execute_reply.started":"2025-05-12T11:23:02.292166Z","shell.execute_reply":"2025-05-12T11:23:02.296224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"patch_size = 256\n\nprint(\"Rozmiar image3:\", image3.shape)\n\nh, w, _ = image3.shape\npatches_vertical = h // patch_size\npatches_horizontal = w // patch_size\n\nprint(f\"Number of patches vertical: {patches_vertical}\")\nprint(f\"Number of patches horizontal: {patches_horizontal}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:23:11.296593Z","iopub.execute_input":"2025-05-12T11:23:11.296907Z","iopub.status.idle":"2025-05-12T11:23:11.302022Z","shell.execute_reply.started":"2025-05-12T11:23:11.296882Z","shell.execute_reply":"2025-05-12T11:23:11.301197Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_patches = patchify(image3, (image_patch_size, image_patch_size, 3), step=image_patch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:23:16.372928Z","iopub.execute_input":"2025-05-12T11:23:16.373227Z","iopub.status.idle":"2025-05-12T11:23:16.377667Z","shell.execute_reply.started":"2025-05-12T11:23:16.373205Z","shell.execute_reply":"2025-05-12T11:23:16.376750Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_patches.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:23:18.931449Z","iopub.execute_input":"2025-05-12T11:23:18.931761Z","iopub.status.idle":"2025-05-12T11:23:18.937265Z","shell.execute_reply.started":"2025-05-12T11:23:18.931738Z","shell.execute_reply":"2025-05-12T11:23:18.936352Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The value of image_patches.shape as (2, 3, 1, 256, 256, 3) means that image_patches is a NumPy array with six dimensions. Each of these dimensions refers to a different aspect of the data that the array stores.\n\nSyntax: (2, 3, 1, 256, 256, 3)\nFirst dimension (2):\n\nThis is the number of \"vertical\" blocks (slices) of the image. This could be the number of rows in the patch grid that were created after the image was split. In this case, this means that the image was split into 2 pieces vertically.\n\nSecond dimension (3):\n\nThis is the number of \"horizontal\" blocks (slices) of the image. This means that the image was split into 3 pieces horizontally.\n\nThird dimension (1):\nIn this case, there is only one image, so it is not a collection of multiple images. This dimension represents the number of sets of images.\n\nFourth dimension (256):\n\nThis is the height of each patch (image fragment). In this case, each patch will be 256 pixels high.\n\nFifth dimension (256):\n\nThis is the width of each patch (image fragment). Each patch is 256 pixels wide.\n\nSixth dimension (3):\n\nThis is the number of color channels. In this case, we have 3 channels, meaning the image is in RGB (red, green, blue) color.","metadata":{}},{"cell_type":"code","source":"# image_patches.shape -> (3, 2, 1, 256, 256, 3)\n\npatches_height = image_patches.shape[0]\npatches_width = image_patches.shape[1]\n\nfig, axs = plt.subplots(patches_height, patches_width, figsize=(patches_width * 3, patches_height * 3))\n\nfor i in range(patches_height):\n    for j in range(patches_width):\n        \n        patch = image_patches[i, j, 0, :, :, :]\n        axs[i, j].imshow(patch)\n        axs[i, j].set_title(f'Patch ({i}, {j})')\n        axs[i, j].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:23:44.774034Z","iopub.execute_input":"2025-05-12T11:23:44.774690Z","iopub.status.idle":"2025-05-12T11:23:45.726507Z","shell.execute_reply.started":"2025-05-12T11:23:44.774661Z","shell.execute_reply":"2025-05-12T11:23:45.725717Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The code image_x = image_patches[0, 0, :, :]  selects the first patch from image_patches with dimensions of 256x256x3 and assigns it to the variable image_x, which contains a fragment of the image in the form of a 256x256x3 matrix (that is, an RGB color image with a resolution of 256x256 px).","metadata":{}},{"cell_type":"code","source":"image_x = image_patches[0,0,:,:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:23:52.702904Z","iopub.execute_input":"2025-05-12T11:23:52.703562Z","iopub.status.idle":"2025-05-12T11:23:52.707470Z","shell.execute_reply.started":"2025-05-12T11:23:52.703529Z","shell.execute_reply":"2025-05-12T11:23:52.706651Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_x.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:23:55.394602Z","iopub.execute_input":"2025-05-12T11:23:55.395222Z","iopub.status.idle":"2025-05-12T11:23:55.400366Z","shell.execute_reply.started":"2025-05-12T11:23:55.395190Z","shell.execute_reply":"2025-05-12T11:23:55.399528Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:29:03.811817Z","iopub.execute_input":"2025-05-12T11:29:03.812521Z","iopub.status.idle":"2025-05-12T11:29:03.818383Z","shell.execute_reply.started":"2025-05-12T11:29:03.812488Z","shell.execute_reply":"2025-05-12T11:29:03.817624Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"minmax_scaler = MinMaxScaler()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:29:08.391529Z","iopub.execute_input":"2025-05-12T11:29:08.392136Z","iopub.status.idle":"2025-05-12T11:29:08.396005Z","shell.execute_reply.started":"2025-05-12T11:29:08.392108Z","shell.execute_reply":"2025-05-12T11:29:08.395047Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. Flattens the image image_x into an array of dimensions (256*256, 3).\n\n2. Scales the pixel values of the image to the range [0, 1] using MinMaxScaler.\n\n3. Then restores the original shape of the image (256, 256, 3) with the scaled pixel values.","metadata":{}},{"cell_type":"code","source":"image_y = minmax_scaler.fit_transform(image_x.reshape(-1, image_x.shape[-1])).reshape(image_x.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:29:11.379111Z","iopub.execute_input":"2025-05-12T11:29:11.379878Z","iopub.status.idle":"2025-05-12T11:29:11.392071Z","shell.execute_reply.started":"2025-05-12T11:29:11.379844Z","shell.execute_reply":"2025-05-12T11:29:11.391234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_y.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:29:13.891741Z","iopub.execute_input":"2025-05-12T11:29:13.892550Z","iopub.status.idle":"2025-05-12T11:29:13.897839Z","shell.execute_reply.started":"2025-05-12T11:29:13.892516Z","shell.execute_reply":"2025-05-12T11:29:13.896879Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:29:16.691079Z","iopub.execute_input":"2025-05-12T11:29:16.691715Z","iopub.status.idle":"2025-05-12T11:29:16.697944Z","shell.execute_reply.started":"2025-05-12T11:29:16.691687Z","shell.execute_reply":"2025-05-12T11:29:16.696944Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# image_patch_size = 256  # Jeśli nie masz zdefiniowanego, dodaj tę linię\n\n# dataset_root_folder = \"/kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T17:53:08.247280Z","iopub.execute_input":"2025-05-11T17:53:08.248129Z","iopub.status.idle":"2025-05-11T17:53:08.252449Z","shell.execute_reply.started":"2025-05-11T17:53:08.248100Z","shell.execute_reply":"2025-05-11T17:53:08.251495Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_dataset = []\nmask_dataset = []\n\nfor image_type in ['images', 'masks']:\n    image_extension = 'jpg' if image_type == 'images' else 'png'\n\n    for tile_id in range(1, 8):  \n        for image_id in range(1, 10): \n            image_path = f\"{dataset_root_folder}/Tile {tile_id}/{image_type}/image_part_{image_id:03d}.{image_extension}\"\n\n            \n            if os.path.exists(image_path):\n                image = cv2.imread(image_path, 1)\n\n                if image is not None:\n                    if image_type == 'masks':\n                        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n                    size_x = (image.shape[1] // image_patch_size) * image_patch_size\n                    size_y = (image.shape[0] // image_patch_size) * image_patch_size\n\n                    image = Image.fromarray(image)\n                    image = image.crop((0, 0, size_x, size_y))\n                    image = np.array(image)\n\n                    patched_images = patchify(image, (image_patch_size, image_patch_size, 3), step=image_patch_size)\n\n                    for i in range(patched_images.shape[0]):\n                        for j in range(patched_images.shape[1]):\n                            individual_patched_image = patched_images[i, j, :, :]\n\n                            if image_type == 'images':\n                                minmax_scaler = MinMaxScaler()\n                                individual_patched_image = minmax_scaler.fit_transform(\n                                    individual_patched_image.reshape(-1, individual_patched_image.shape[-1])\n                                ).reshape(individual_patched_image.shape)\n\n                                individual_patched_image = individual_patched_image[0]\n                                image_dataset.append(individual_patched_image)\n\n                            elif image_type == 'masks':\n                                individual_patched_image = individual_patched_image[0]\n                                mask_dataset.append(individual_patched_image)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:29:29.975656Z","iopub.execute_input":"2025-05-12T11:29:29.976540Z","iopub.status.idle":"2025-05-12T11:29:40.127896Z","shell.execute_reply.started":"2025-05-12T11:29:29.976509Z","shell.execute_reply":"2025-05-12T11:29:40.127312Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(image_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:34:42.344078Z","iopub.execute_input":"2025-05-12T11:34:42.344920Z","iopub.status.idle":"2025-05-12T11:34:42.350623Z","shell.execute_reply.started":"2025-05-12T11:34:42.344886Z","shell.execute_reply":"2025-05-12T11:34:42.349460Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(mask_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:34:45.912135Z","iopub.execute_input":"2025-05-12T11:34:45.912810Z","iopub.status.idle":"2025-05-12T11:34:45.918115Z","shell.execute_reply.started":"2025-05-12T11:34:45.912778Z","shell.execute_reply":"2025-05-12T11:34:45.917155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_dataset = np.array(image_dataset)\nmask_dataset = np.array(mask_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:34:52.778061Z","iopub.execute_input":"2025-05-12T11:34:52.778375Z","iopub.status.idle":"2025-05-12T11:34:53.246160Z","shell.execute_reply.started":"2025-05-12T11:34:52.778351Z","shell.execute_reply":"2025-05-12T11:34:53.245225Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(image_dataset.shape)\nprint(mask_dataset.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:34:56.931017Z","iopub.execute_input":"2025-05-12T11:34:56.931838Z","iopub.status.idle":"2025-05-12T11:34:56.936006Z","shell.execute_reply.started":"2025-05-12T11:34:56.931806Z","shell.execute_reply":"2025-05-12T11:34:56.935228Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_dataset[0].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:34:59.678600Z","iopub.execute_input":"2025-05-12T11:34:59.678913Z","iopub.status.idle":"2025-05-12T11:34:59.684906Z","shell.execute_reply.started":"2025-05-12T11:34:59.678891Z","shell.execute_reply":"2025-05-12T11:34:59.684011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random_image_id = random.randint(0, len(image_dataset)-1)\n\nplt.figure(figsize=(10,10))\nplt.subplot(1,2,1)\nplt.imshow(image_dataset[random_image_id])\nplt.subplot(1,2,2)\nplt.imshow(mask_dataset[random_image_id])\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:35:02.772348Z","iopub.execute_input":"2025-05-12T11:35:02.773304Z","iopub.status.idle":"2025-05-12T11:35:03.184752Z","shell.execute_reply.started":"2025-05-12T11:35:02.773252Z","shell.execute_reply":"2025-05-12T11:35:03.183781Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Converting a color in hexadecimal (HEX) format to RGB (Red, Green, Blue) values, which are used in analysis such as image processing. The HEX format is commonly used to specify colors in graphics applications and web pages. Each color in this format is represented by a string of six characters, where the first two are the Red value, the next two are the Green value, and the last two are the Blue value.\n\nConversion from hexadecimal (HEX) to decimal (RGB):\nEach color in HEX format (e.g. '#3C1098') is a string of six characters.\n\nThe first two digits represent the red component of the color, the next two are the green component, and the last two are the blue component.\n\nHexadecimal (base 16) values ​​are converted to decimal numbers.\nExample for color #3C1098:\n\n'3C' is the hexadecimal value that converts to decimal, which is 60 (red component).\n\n'10' is the hexadecimal value that converts to 16 (green component).\n\n'98' is the hexadecimal value that converts to 152 (blue component).\n\nTherefore, for #3C1098, the resulting RGB value is [60, 16, 152].\nFor each color we convert HEX to RGB:\nBuilding = '#3C1098' – converts to [60, 16, 152]\nLand = '#8429F6' – converts to [132, 41, 246]\nRoad = '#6EC1E4' – converts to [110, 193, 228]\nVegetation = 'FEDD3A' – converts to [254, 221, 58]\nWater = 'E2A929' – converts to [226, 169, 41]\nUnlabeled = '#9B9B9B' – converts to [155, 155, 155]","metadata":{}},{"cell_type":"code","source":"\na=int('3C', 16)  #3C with base 16. Should return 60.\nprint(a)\n#Do the same for all RGB channels in each hex code to convert to RGB\nBuilding = '#3C1098'.lstrip('#')\nBuilding = np.array(tuple(int(Building[i:i+2], 16) for i in (0, 2, 4))) # 60, 16, 152\n\nLand = '#8429F6'.lstrip('#')\nLand = np.array(tuple(int(Land[i:i+2], 16) for i in (0, 2, 4))) #132, 41, 246\n\nRoad = '#6EC1E4'.lstrip('#')\nRoad = np.array(tuple(int(Road[i:i+2], 16) for i in (0, 2, 4))) #110, 193, 228\n\nVegetation =  'FEDD3A'.lstrip('#')\nVegetation = np.array(tuple(int(Vegetation[i:i+2], 16) for i in (0, 2, 4))) #254, 221, 58\n\nWater = 'E2A929'.lstrip('#')\nWater = np.array(tuple(int(Water[i:i+2], 16) for i in (0, 2, 4))) #226, 169, 41\n\nUnlabeled = '#9B9B9B'.lstrip('#')\nUnlabeled = np.array(tuple(int(Unlabeled[i:i+2], 16) for i in (0, 2, 4))) #155, 155, 155\n\nlabel = individual_patched_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:35:24.211178Z","iopub.execute_input":"2025-05-12T11:35:24.211875Z","iopub.status.idle":"2025-05-12T11:35:24.219702Z","shell.execute_reply.started":"2025-05-12T11:35:24.211841Z","shell.execute_reply":"2025-05-12T11:35:24.218791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_number = random.randint(0, len(image_dataset))\nplt.figure(figsize=(12, 6))\nplt.subplot(121)\nplt.imshow(np.reshape(image_dataset[image_number], (patch_size, patch_size, 3)))\nplt.subplot(122)\nplt.imshow(np.reshape(mask_dataset[image_number], (patch_size, patch_size, 3)))\nplt.show()","metadata":{"id":"oIMGo8h0y7wT","execution":{"iopub.status.busy":"2025-05-12T11:35:27.531320Z","iopub.execute_input":"2025-05-12T11:35:27.531993Z","iopub.status.idle":"2025-05-12T11:35:27.993986Z","shell.execute_reply.started":"2025-05-12T11:35:27.531962Z","shell.execute_reply":"2025-05-12T11:35:27.992960Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The rgb_to_2D_label(label) function is for converting an image (mask) in RGB format to a 2D image that contains numeric labels instead of colors. Instead of three channels (RGB), the function maps each color to a specific numeric value (label assignment step).","metadata":{}},{"cell_type":"markdown","source":"Input: This function takes an RGB label image, where each pixel is represented by a vector (e.g. [60, 16, 152] for the color Building).\n\nCreate an empty array label_seg: Creates an empty array of the same size as the input label image, with values ​​set to 0 (e.g. uint8).\nConvert RGB values ​​to numeric labels: Then, for each pixel in the image, checks if its color matches one of the colors in the defined variables (such as Building, Land, Road, etc.). If a pixel has a specific color, it assigns a corresponding numeric label to it:\n\nBuilding (RGB: [60, 16, 152]) -> label 0\n\nLand (RGB: [132, 41, 246]) -> label 1\n\nRoad (RGB: [110, 193, 228]) -> label 2\n\nVegetation (RGB: [254, 221, 58]) -> label 3\n\nWater (RGB: [226, 169, 41]) -> label 4\n\nUnlabeled (RGB: [155, 155, 155]) -> label 5\n\nEach of these colors is assigned a corresponding number (0, 1, 2, 3, 4, 5).","metadata":{}},{"cell_type":"code","source":"def rgb_to_2D_label(label):\n    \"\"\"\n    Suply our labale masks as input in RGB format.\n    Replace pixels with specific RGB values ...\n    \"\"\"\n    label_seg = np.zeros(label.shape,dtype=np.uint8)\n    label_seg [np.all(label == Building,axis=-1)] = 0\n    label_seg [np.all(label==Land,axis=-1)] = 1\n    label_seg [np.all(label==Road,axis=-1)] = 2\n    label_seg [np.all(label==Vegetation,axis=-1)] = 3\n    label_seg [np.all(label==Water,axis=-1)] = 4\n    label_seg [np.all(label==Unlabeled,axis=-1)] = 5\n\n    label_seg = label_seg[:,:,0] \n\n    return label_seg","metadata":{"id":"5W4s8DzcEAok","execution":{"iopub.status.busy":"2025-05-12T11:35:38.131181Z","iopub.execute_input":"2025-05-12T11:35:38.131620Z","iopub.status.idle":"2025-05-12T11:35:38.137716Z","shell.execute_reply.started":"2025-05-12T11:35:38.131592Z","shell.execute_reply":"2025-05-12T11:35:38.136774Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels = []\nfor i in range(mask_dataset.shape[0]):\n    label = rgb_to_2D_label(mask_dataset[i])\n    labels.append(label)\n\nlabels = np.array(labels)\nlabels = np.expand_dims(labels, axis=3)\n\n\nprint(\"Unique labels in label dataset are: \", np.unique(labels))","metadata":{"id":"VTMxjlEnEDqt","execution":{"iopub.status.busy":"2025-05-12T11:35:43.151136Z","iopub.execute_input":"2025-05-12T11:35:43.151789Z","iopub.status.idle":"2025-05-12T11:35:56.640948Z","shell.execute_reply.started":"2025-05-12T11:35:43.151759Z","shell.execute_reply":"2025-05-12T11:35:56.640063Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_number = random.randint(0, len(image_dataset))\nplt.figure(figsize=(12, 6))\nplt.subplot(121)\nplt.imshow(image_dataset[image_number])\nplt.subplot(122)\nplt.imshow(labels[image_number][:,:,0])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-12T11:37:22.942673Z","iopub.execute_input":"2025-05-12T11:37:22.943276Z","iopub.status.idle":"2025-05-12T11:37:23.412294Z","shell.execute_reply.started":"2025-05-12T11:37:22.943244Z","shell.execute_reply":"2025-05-12T11:37:23.411442Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_classes = len(np.unique(labels))\nfrom keras.utils import to_categorical\nlabels_cat = to_categorical(labels, num_classes=n_classes)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(image_dataset, labels_cat, test_size = 0.20, random_state = 42)\n","metadata":{"id":"eaE9K40lEOHl","execution":{"iopub.status.busy":"2025-05-12T11:37:43.891652Z","iopub.execute_input":"2025-05-12T11:37:43.891972Z","iopub.status.idle":"2025-05-12T11:37:49.097701Z","shell.execute_reply.started":"2025-05-12T11:37:43.891947Z","shell.execute_reply":"2025-05-12T11:37:49.096653Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"weights = [0.1666, 0.1666, 0.1666, 0.1666, 0.1666, 0.1666]\ndice_loss = sm.losses.DiceLoss(class_weights=weights)\nfocal_loss = sm.losses.CategoricalFocalLoss()\ntotal_loss = dice_loss + (1 * focal_loss)","metadata":{"id":"2giPD9L7ETpE","execution":{"iopub.status.busy":"2025-05-12T11:37:49.099484Z","iopub.execute_input":"2025-05-12T11:37:49.099785Z","iopub.status.idle":"2025-05-12T11:37:49.104428Z","shell.execute_reply.started":"2025-05-12T11:37:49.099761Z","shell.execute_reply":"2025-05-12T11:37:49.103605Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMG_HEIGHT = X_train.shape[1]\nIMG_WIDTH  = X_train.shape[2]\nIMG_CHANNELS = X_train.shape[3]","metadata":{"id":"r4ISYA_sEUPE","execution":{"iopub.status.busy":"2025-05-12T11:37:52.690309Z","iopub.execute_input":"2025-05-12T11:37:52.690631Z","iopub.status.idle":"2025-05-12T11:37:52.694745Z","shell.execute_reply.started":"2025-05-12T11:37:52.690609Z","shell.execute_reply":"2025-05-12T11:37:52.693779Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\nfrom keras import backend as K","metadata":{"id":"tsMBzx8f_5wD","execution":{"iopub.status.busy":"2025-05-12T11:37:55.056275Z","iopub.execute_input":"2025-05-12T11:37:55.057125Z","iopub.status.idle":"2025-05-12T11:37:55.062613Z","shell.execute_reply.started":"2025-05-12T11:37:55.057097Z","shell.execute_reply":"2025-05-12T11:37:55.061797Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def jacard_coef(y_true, y_pred):\n    y_true_f = tf.keras.backend.flatten(y_true)\n    y_pred_f = tf.keras.backend.flatten(y_pred)\n    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n    return (intersection + 1.0) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) - intersection + 1.0)","metadata":{"id":"hX1uFS2h_84M","execution":{"iopub.status.busy":"2025-05-12T11:37:59.242175Z","iopub.execute_input":"2025-05-12T11:37:59.242861Z","iopub.status.idle":"2025-05-12T11:37:59.247869Z","shell.execute_reply.started":"2025-05-12T11:37:59.242829Z","shell.execute_reply":"2025-05-12T11:37:59.246842Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 04. Build the U-Net model ","metadata":{}},{"cell_type":"code","source":"def multi_unet_model(n_classes=4, IMG_HEIGHT=256, IMG_WIDTH=256, IMG_CHANNELS=1):\n    \n    inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n    \n    s = inputs\n\n    #Contraction path\n    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n    c1 = Dropout(0.2)(c1)  # Original 0.1\n    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n    p1 = MaxPooling2D((2, 2))(c1)\n\n    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n    c2 = Dropout(0.2)(c2)  # Original 0.1\n    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n    p2 = MaxPooling2D((2, 2))(c2)\n\n    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n    c3 = Dropout(0.2)(c3)\n    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n    p3 = MaxPooling2D((2, 2))(c3)\n\n    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n    c4 = Dropout(0.2)(c4)\n    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n\n    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n    c5 = Dropout(0.3)(c5)\n    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n\n    #Expansive path\n    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n    u6 = concatenate([u6, c4])\n    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n    c6 = Dropout(0.2)(c6)\n    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n\n    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n    u7 = concatenate([u7, c3])\n    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n    c7 = Dropout(0.2)(c7)\n    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n\n    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n    u8 = concatenate([u8, c2])\n    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n    c8 = Dropout(0.2)(c8)  # Original 0.1\n    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n\n    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n    u9 = concatenate([u9, c1], axis=3)\n    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n    c9 = Dropout(0.2)(c9)  # Original 0.1\n    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n\n    outputs = Conv2D(n_classes, (1, 1), activation='softmax')(c9)\n\n    model = Model(inputs=[inputs], outputs=[outputs])\n\n    return model","metadata":{"id":"8VAEYJMu__4x","execution":{"iopub.status.busy":"2025-05-12T11:38:13.336573Z","iopub.execute_input":"2025-05-12T11:38:13.337140Z","iopub.status.idle":"2025-05-12T11:38:13.351825Z","shell.execute_reply.started":"2025-05-12T11:38:13.337113Z","shell.execute_reply":"2025-05-12T11:38:13.350906Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metrics=['accuracy', jacard_coef]","metadata":{"id":"h2lRHywTEXBW","execution":{"iopub.status.busy":"2025-05-12T11:38:20.930687Z","iopub.execute_input":"2025-05-12T11:38:20.931372Z","iopub.status.idle":"2025-05-12T11:38:20.935291Z","shell.execute_reply.started":"2025-05-12T11:38:20.931345Z","shell.execute_reply":"2025-05-12T11:38:20.934435Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_model():\n    return multi_unet_model(n_classes=6, IMG_HEIGHT=256, IMG_WIDTH=256, IMG_CHANNELS=3)","metadata":{"id":"k70fOeZfEZkl","execution":{"iopub.status.busy":"2025-05-12T11:38:25.505640Z","iopub.execute_input":"2025-05-12T11:38:25.506217Z","iopub.status.idle":"2025-05-12T11:38:25.510238Z","shell.execute_reply.started":"2025-05-12T11:38:25.506190Z","shell.execute_reply":"2025-05-12T11:38:25.509312Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = get_model()\nmodel.compile(optimizer='adam', loss=total_loss, metrics=metrics)\nmodel.summary()","metadata":{"editable":true,"id":"UbpHkHEqEdaM","slideshow":{"slide_type":""},"tags":[],"execution":{"iopub.status.busy":"2025-05-12T11:38:28.731907Z","iopub.execute_input":"2025-05-12T11:38:28.732701Z","iopub.status.idle":"2025-05-12T11:38:29.272111Z","shell.execute_reply.started":"2025-05-12T11:38:28.732671Z","shell.execute_reply":"2025-05-12T11:38:29.271158Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 05. Minor adjustments to avoid Kaggle T4-x2 GPU errors (Optional)","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import backend as K\n\n# Set a specific convolution algorithm\nconfig = tf.compat.v1.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.force_gpu_compatible = True\nsession = tf.compat.v1.Session(config=config)\n","metadata":{"execution":{"iopub.status.busy":"2025-05-12T11:46:51.180206Z","iopub.execute_input":"2025-05-12T11:46:51.180939Z","iopub.status.idle":"2025-05-12T11:46:51.188800Z","shell.execute_reply.started":"2025-05-12T11:46:51.180906Z","shell.execute_reply":"2025-05-12T11:46:51.187947Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import mixed_precision\n\n# Set the global policy to mixed precision\npolicy = mixed_precision.Policy('mixed_float16')\nmixed_precision.set_global_policy(policy)\n\n# Verify the policy\nprint(f'Mixed precision policy: {tf.keras.mixed_precision.global_policy()}')\n","metadata":{"execution":{"iopub.status.busy":"2025-05-12T11:46:54.101769Z","iopub.execute_input":"2025-05-12T11:46:54.102428Z","iopub.status.idle":"2025-05-12T11:46:54.107489Z","shell.execute_reply.started":"2025-05-12T11:46:54.102375Z","shell.execute_reply":"2025-05-12T11:46:54.106467Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\ntf.debugging.set_log_device_placement(True)","metadata":{"execution":{"iopub.status.busy":"2025-05-12T11:46:58.131110Z","iopub.execute_input":"2025-05-12T11:46:58.131890Z","iopub.status.idle":"2025-05-12T11:46:58.135566Z","shell.execute_reply.started":"2025-05-12T11:46:58.131859Z","shell.execute_reply":"2025-05-12T11:46:58.134882Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 06. Train U-Net model with the preprocessed dataset","metadata":{}},{"cell_type":"code","source":"\nhistory1 = model.fit(X_train, y_train, \n                    batch_size = 16, \n                    verbose=1, \n                    epochs=50, \n                    validation_data=(X_test, y_test), \n                    shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2025-05-12T11:47:01.496437Z","iopub.execute_input":"2025-05-12T11:47:01.497076Z","iopub.status.idle":"2025-05-12T11:54:40.601767Z","shell.execute_reply.started":"2025-05-12T11:47:01.497046Z","shell.execute_reply":"2025-05-12T11:54:40.600763Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 07. Save trained model to Kaggle Output","metadata":{}},{"cell_type":"code","source":"model.save(\"/kaggle/working/satellite_standard_unet_100epochs.hdf5\")","metadata":{"execution":{"iopub.status.busy":"2025-05-11T18:44:40.788842Z","iopub.execute_input":"2025-05-11T18:44:40.789933Z","iopub.status.idle":"2025-05-11T18:44:40.952995Z","shell.execute_reply.started":"2025-05-11T18:44:40.789899Z","shell.execute_reply":"2025-05-11T18:44:40.952034Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 08. Plot training and validation accuracy and loss at each epoch","metadata":{}},{"cell_type":"code","source":"history = history1\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, loss, 'y', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nacc = history.history['jacard_coef']\nval_acc = history.history['val_jacard_coef']\n\nplt.plot(epochs, acc, 'y', label='Training IoU')\nplt.plot(epochs, val_acc, 'r', label='Validation IoU')\nplt.title('Training and validation IoU')\nplt.xlabel('Epochs')\nplt.ylabel('IoU')\nplt.legend()\nplt.show()","metadata":{"id":"hgtf2tR7ErF1","execution":{"iopub.status.busy":"2025-05-11T18:44:46.272395Z","iopub.execute_input":"2025-05-11T18:44:46.273216Z","iopub.status.idle":"2025-05-11T18:44:46.759810Z","shell.execute_reply.started":"2025-05-11T18:44:46.273175Z","shell.execute_reply":"2025-05-11T18:44:46.758728Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 09. Predict Using Saved Model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\nimport os\nos.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n\nfrom tensorflow.keras.models import load_model\nimport segmentation_models as sm\nfrom tensorflow.keras import backend as K","metadata":{"execution":{"iopub.status.busy":"2025-05-11T18:44:53.567605Z","iopub.execute_input":"2025-05-11T18:44:53.568422Z","iopub.status.idle":"2025-05-11T18:44:53.580850Z","shell.execute_reply.started":"2025-05-11T18:44:53.568389Z","shell.execute_reply":"2025-05-11T18:44:53.580000Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the custom loss functions\nweights = [0.1666, 0.1666, 0.1666, 0.1666, 0.1666, 0.1666]\ndice_loss = sm.losses.DiceLoss(class_weights=weights)\nfocal_loss = sm.losses.CategoricalFocalLoss()\ntotal_loss = dice_loss + (1 * focal_loss)  # Composite loss function","metadata":{"execution":{"iopub.status.busy":"2025-05-12T11:57:08.691113Z","iopub.execute_input":"2025-05-12T11:57:08.699852Z","iopub.status.idle":"2025-05-12T11:57:08.704947Z","shell.execute_reply.started":"2025-05-12T11:57:08.699818Z","shell.execute_reply":"2025-05-12T11:57:08.704030Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def jacard_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)","metadata":{"execution":{"iopub.status.busy":"2025-05-11T18:45:02.768918Z","iopub.execute_input":"2025-05-11T18:45:02.769682Z","iopub.status.idle":"2025-05-11T18:45:02.775285Z","shell.execute_reply.started":"2025-05-11T18:45:02.769652Z","shell.execute_reply":"2025-05-11T18:45:02.774359Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Path to model\nmodel_path = \"/kaggle/working/satellite_standard_unet_100epochs.hdf5\"","metadata":{"execution":{"iopub.status.busy":"2025-05-11T18:45:06.593785Z","iopub.execute_input":"2025-05-11T18:45:06.594451Z","iopub.status.idle":"2025-05-11T18:45:06.598041Z","shell.execute_reply.started":"2025-05-11T18:45:06.594422Z","shell.execute_reply":"2025-05-11T18:45:06.597218Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"custom_objects = {\n    \"dice_loss_plus_1focal_loss\": total_loss,\n    \"jacard_coef\": jacard_coef\n}\n\n# Load the model with custom objects\nmodel = load_model(model_path, custom_objects=custom_objects)","metadata":{"execution":{"iopub.status.busy":"2025-05-11T18:45:09.639761Z","iopub.execute_input":"2025-05-11T18:45:09.640210Z","iopub.status.idle":"2025-05-11T18:45:09.872250Z","shell.execute_reply.started":"2025-05-11T18:45:09.640149Z","shell.execute_reply":"2025-05-11T18:45:09.871339Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"IoU (Intersection over Union) is a popular metric used to evaluate the quality of image segmentation, especially in tasks such as semantic segmentation with the U-Net model. IoU measures the overlap between predicted (prediction) and actual (ground truth) class masks.\nIntersection = number of pixels that are correctly classified (both y_pred and y_true have the same class value).\n\nUnion = number of pixels that are classified as a given class in y_pred or y_true.","metadata":{}},{"cell_type":"markdown","source":"y_pred – model prediction (has shape: [batch, height, width, num_classes]).\n\nnp.argmax(..., axis=3) – converts predictions from one-hot or softmax to classes (0, 1, 2, etc.), i.e. selects the most probable class for each pixel.\n\ny_pred_argmax and y_test_argmax – these are now 2D class maps (for each image).","metadata":{}},{"cell_type":"code","source":"#IOU\ny_pred=model.predict(X_test)\ny_pred_argmax=np.argmax(y_pred, axis=3)\ny_test_argmax=np.argmax(y_test, axis=3)","metadata":{"execution":{"iopub.status.busy":"2025-05-11T18:45:13.232630Z","iopub.execute_input":"2025-05-11T18:45:13.232955Z","iopub.status.idle":"2025-05-11T18:45:48.821990Z","shell.execute_reply.started":"2025-05-11T18:45:13.232931Z","shell.execute_reply":"2025-05-11T18:45:48.821280Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Using built in keras function for IoU\nfrom keras.metrics import MeanIoU\nn_classes = 6\nIOU_keras = MeanIoU(num_classes=n_classes)\nIOU_keras.update_state(y_test_argmax, y_pred_argmax)\nprint(\"Mean IoU =\", IOU_keras.result().numpy())","metadata":{"execution":{"iopub.status.busy":"2025-05-11T18:45:54.412706Z","iopub.execute_input":"2025-05-11T18:45:54.413306Z","iopub.status.idle":"2025-05-11T18:45:55.933732Z","shell.execute_reply.started":"2025-05-11T18:45:54.413274Z","shell.execute_reply":"2025-05-11T18:45:55.932796Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"IoU (Intersection over Union) is a metric used to evaluate segmentation performance by comparing the predicted area to the ground truth area:\n\nIoU = Intersection/Union = TP/TP + FP + FN\n \nTP (True Positive): pixels correctly classified as a given class\n\nFP (False Positive): pixels incorrectly predicted as that class\n\nFN (False Negative): actual pixels of the class that were missed\n\n","metadata":{}},{"cell_type":"markdown","source":"Mean IoU Range\tSegmentation Quality\n0.90 – 1.00\tExcellent\n0.75 – 0.90\tVery good\n0.50 – 0.75\tDecent / Acceptable\n0.25 – 0.50\tWeak\n0.00 – 0.25\tVery poor","metadata":{}},{"cell_type":"code","source":"import random\ntest_img_number = random.randint(0, len(X_test))\ntest_img = X_test[test_img_number]\nground_truth=y_test_argmax[test_img_number]\ntest_img_input=np.expand_dims(test_img, 0)\nprediction = (model.predict(test_img_input))\npredicted_img=np.argmax(prediction, axis=3)[0,:,:]\n\n\nplt.figure(figsize=(12, 8))\nplt.subplot(231)\nplt.title('Testing Image')\nplt.imshow(test_img)\nplt.subplot(232)\nplt.title('Testing Label')\nplt.imshow(ground_truth)\nplt.subplot(233)\nplt.title('Prediction on test image')\nplt.imshow(predicted_img)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-11T18:45:58.915622Z","iopub.execute_input":"2025-05-11T18:45:58.915954Z","iopub.status.idle":"2025-05-11T18:46:02.817052Z","shell.execute_reply.started":"2025-05-11T18:45:58.915928Z","shell.execute_reply":"2025-05-11T18:46:02.816197Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Number of images to predict\nnum_images = 5\n\n# Plot setup\nplt.figure(figsize=(20, 20))\n\nfor i in range(num_images):\n    test_img_number = random.randint(0, len(X_test))\n    test_img = X_test[test_img_number]\n    ground_truth=y_test_argmax[test_img_number]\n    test_img_input=np.expand_dims(test_img, 0)\n    prediction = (model.predict(test_img_input))\n    predicted_img=np.argmax(prediction, axis=3)[0,:,:]\n\n    plt.figure(figsize=(12, 8))\n    plt.subplot(231)\n    plt.title('Testing Image')\n    plt.imshow(test_img)\n    plt.subplot(232)\n    plt.title('Testing Label')\n    plt.imshow(ground_truth)\n    plt.subplot(233)\n    plt.title('Prediction on test image')\n    plt.imshow(predicted_img)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-11T18:46:09.161037Z","iopub.execute_input":"2025-05-11T18:46:09.161871Z","iopub.status.idle":"2025-05-11T18:46:11.825678Z","shell.execute_reply.started":"2025-05-11T18:46:09.161838Z","shell.execute_reply":"2025-05-11T18:46:11.824858Z"},"trusted":true},"outputs":[],"execution_count":null}]}